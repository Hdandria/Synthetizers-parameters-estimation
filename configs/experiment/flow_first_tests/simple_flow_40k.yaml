# @package _global_

# Experiment config combining train_40k.yaml structure with flow_simple.yaml hyperparameters
# Updated dataset paths to use datasets/experiment_1

defaults:
  - override /data: surge
  - override /model: surge_flow
  - override /callbacks: default_surge
  - override /trainer: gpu
  - override /logger: many_loggers
  - override /paths: default
  - override /extras: default
  - override /hydra: default

# Core training configuration
task_name: train
run_name: flow
tags:
  - surge
train: true
test: true
ckpt_path: null
seed: 3123123

# Data configuration
data:
  _target_: src.data.surge_datamodule.SurgeDataModule
  dataset_root: datasets/experiment_1
  use_saved_mean_and_variance: true
  batch_size: 128
  ot: true
  num_workers: 11
  predict_file: datasets/experiment_1/test.h5

# Model configuration
model:
  encoder:
    _target_: src.models.components.transformer.AudioSpectrogramTransformer
    d_model: 512
    n_heads: 8
    n_layers: 8
    n_conditioning_outputs: 8
    patch_size: 16
    patch_stride: 10
    input_channels: 2
    spec_shape:
      - 128
      - 401
  _target_: src.models.surge_flow_matching_module.SurgeFlowMatchingModule
  warmup_steps: 0
  scheduler:
    _target_: torch.optim.lr_scheduler.CosineAnnealingLR
    _partial_: true
    T_max: ${trainer.max_steps}
    eta_min: ${mul:${model.optimizer.lr},1e-2}
  optimizer:
    _target_: torch.optim.Adam
    _partial_: true
    lr: 0.0001
    weight_decay: 0.0
  vector_field:
    _target_: src.models.components.transformer.ApproxEquivTransformer
    projection:
      _target_: src.models.components.transformer.LearntProjection
      d_model: ${model.vector_field.d_model}
      d_token: ${model.vector_field.d_model}
      num_params: ${model.num_params}
      num_tokens: 128
      initial_ffn: true
      final_ffn: false
    num_layers: 8
    d_model: 512
    conditioning_dim: ${model.encoder.d_model}
    num_heads: 8
    d_ff: 512
    num_tokens: ${model.vector_field.projection.num_tokens}
    learn_pe: false
    learn_projection: true
    pe_type: none
    pe_penalty: 0.0
    time_encoding: sinusoidal
    projection_penalty: 0.01
    norm: layer
    skip_first_norm: false
    adaln_mode: basic
    zero_init: false
    outer_residual: false
  num_params: 92
  cfg_dropout_rate: 0.1
  rectified_sigma_min: 0.0
  validation_sample_steps: 50
  validation_cfg_strength: 2.0
  test_sample_steps: 200
  test_cfg_strength: 2.0
  compile: true

# Callbacks configuration
callbacks:
  model_checkpoint:
    _target_: lightning.pytorch.callbacks.ModelCheckpoint
    dirpath: ${paths.output_dir}/checkpoints
    filename: epoch_{epoch:03d}
    monitor: null
    verbose: false
    save_last: true
    save_top_k: 1
    mode: min
    auto_insert_metric_name: false
    save_weights_only: false
    every_n_train_steps: 5000
    train_time_interval: null
    every_n_epochs: null
    save_on_train_epoch_end: null
  lr_monitor:
    _target_: lightning.pytorch.callbacks.LearningRateMonitor
    logging_interval: step
  model_summary:
    _target_: lightning.pytorch.callbacks.RichModelSummary
    max_depth: 2
  rich_progress_bar:
    _target_: lightning.pytorch.callbacks.RichProgressBar
  plot_proj_ii:
    _target_: src.utils.callbacks.PlotLearntProjection
    after_val: false
    every_n_steps: 5000
  early_stopping: null

# Logger configuration
logger:
  csv:
    _target_: lightning.pytorch.loggers.csv_logs.CSVLogger
    save_dir: ${paths.output_dir}
    name: csv/
    prefix: ''
  wandb:
    _target_: lightning.pytorch.loggers.wandb.WandbLogger
    save_dir: ${paths.output_dir}
    offline: false
    id: null
    anonymous: null
    project: synth-prediction
    log_model: true
    prefix: ''
    entity: paindespistes-t-l-com-paris
    group: ${experiment_name}
    tags: ${tags}
    job_type: ''
    settings:
      _target_: wandb.Settings
      code_dir: .
    name: ${experiment_name}_${run_name}

# Trainer configuration
trainer:
  _target_: lightning.pytorch.trainer.Trainer
  default_root_dir: ${paths.output_dir}
  min_steps: 100000
  max_steps: 100000
  log_every_n_steps: 1
  val_check_interval: 2 # limit_val_batches: 0 so this is ignored (no val)
  gradient_clip_val: 0.5
  accelerator: gpu
  devices: 1
  check_val_every_n_epoch: null
  deterministic: false
  limit_val_batches: 0
  precision: bf16-mixed

# Paths configuration
paths:
  root_dir: ${oc.env:PROJECT_ROOT}
  data_dir: ${paths.root_dir}/data/
  log_dir: ${paths.root_dir}/logs/
  output_dir: ${hydra:runtime.output_dir}
  work_dir: ${hydra:runtime.cwd}

# Extras configuration
extras:
  ignore_warnings: false
  enforce_tags: true
  print_config: true
  float32_matmul_precision: high

# Hydra configuration (minimal)
hydra:
  run:
    dir: ${paths.log_dir}/${task_name}/${experiment_name}/${run_name}-${now:%Y-%m-%d}_${now:%H-%M-%S}
  sweep:
    dir: ${paths.log_dir}/${task_name}/multiruns/${now:%Y-%m-%d}_${now:%H-%M-%S}
    subdir: ${hydra.job.num}
  job_logging:
    handlers:
      file:
        filename: ${hydra.runtime.output_dir}/${task_name}.log

# Experiment metadata
experiment_name: simple-flow-40k